{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_from_scratch_2_chapter4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29FUZEB4waqO",
        "outputId": "7396dae2-5e02-4ea1-f757-15354fde1fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deep-learning-from-scratch-2' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch-2.git\n",
        "import sys, os\n",
        "sys.path.append('/content/deep-learning-from-scratch-2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install cupy"
      ],
      "metadata": {
        "id": "MrgcwaF6R7xT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Cm9rcbATyNri"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding 구현\n",
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx] # 가중치W의 특정행만 추출\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0 # dW형상을 유지한 채 그 원소들을 0으로 덮어씀\n",
        "    for i, word_id in enumerate(self.idx):\n",
        "      dW[word_id] += dout[i] # 먼저 쓰여진 값에 덮어써지는 중복문제를 해결\n",
        "    return None"
      ],
      "metadata": {
        "id": "32nUsaA2xd5N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Dot 구현\n",
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx) # Embedding 계층의 foward(idx)를 호출\n",
        "    out = np.sum(target_W * h, axis=1) # 내적계산\n",
        "    self.cache = (h, target_W)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "    dtarget_W = dout * h\n",
        "    self.embed.backward(dtarget_W)\n",
        "    dh = dout * target_W\n",
        "    return dh"
      ],
      "metadata": {
        "id": "3WMO1myszKGy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UnigramSample 구현\n",
        "import collections\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0\n",
        "                p /= p.sum()\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "        else:\n",
        "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
        "            # 부정적 예에 타깃이 포함될 수 있다.\n",
        "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample"
      ],
      "metadata": {
        "id": "ZI-88T2o7s8Y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative Sampling Loss 구현\n",
        "from common.config import GPU\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.layers import SigmoidWithLoss\n",
        "class NegativeSamplingLoss:\n",
        "  def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "    self.sample_size = sample_size\n",
        "    self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "    self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "    self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.embed_dot_layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, h, target):\n",
        "    batch_size = target.shape[0]\n",
        "    negative_sample = self.sampler.get_negative_sample(target)\n",
        "    # 긍정적\n",
        "    score = self.embed_dot_layers[0].forward(h, target) # forward 점수 계산\n",
        "    correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "    loss = self.loss_layers[0].forward(score, correct_label) #손실 계산\n",
        "    # 부정적\n",
        "    negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "    for i in range(self.sample_size):\n",
        "      negative_target = negative_sample[:, i]\n",
        "      score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "      loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dh = 0\n",
        "    for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "      dscore = l0.backward(dout)\n",
        "      dh += l1.backward(dscore)\n",
        "    return dh"
      ],
      "metadata": {
        "id": "yNdU3QDS1I0R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW 모델 구현\n",
        "class CBOW:\n",
        "  def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "    V, H = vocab_size, hidden_size\n",
        "\n",
        "    # 가중치 초기화\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "    W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "    # 계층생성\n",
        "    self.in_layers = []\n",
        "    for i in range(2*window_size):\n",
        "      layer = Embedding(W_in) # Embedding 계층 생성\n",
        "      self.in_layers.append(layer)\n",
        "    self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "    # 모든 가중치와 기울기를 배열 모음\n",
        "    layers = self.in_layers + [self.ns_loss]\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "    # 인스턴스 변수에 단어의 분산 표현을 저장\n",
        "    self.word_vecs = W_in\n",
        "\n",
        "  def forward(self, contexts, target):\n",
        "    h = 0\n",
        "    for i, layer in enumerate(self.in_layers):\n",
        "      h += layer.forward(contexts[:, i])\n",
        "    h *= 1/len(self.in_layers)\n",
        "    loss = self.ns_loss.forward(h, target)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.ns_loss.backward(dout)\n",
        "    dout *= 1/ len(self.in_layers)\n",
        "    for layer in self.in_layers:\n",
        "      layer.backward(dout)\n",
        "    return None"
      ],
      "metadata": {
        "id": "yVAimNDJ9J7L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model train\n",
        "import pickle\n",
        "from common import config\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb\n",
        "\n",
        "#config.GPU = True\n",
        "\n",
        "# Hyperparameter\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# Data Read\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "\n",
        "#if config.GPU:\n",
        "#  contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "# Model\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# Train\n",
        "trainer.fit(contexts, target, max_epoch, batch_size, eval_interval = 3000)\n",
        "trainer.plot()\n",
        "\n",
        "# Data save\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "  word_vecs = to_cpu(word_vecs)\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file='cbow_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "  pickle.dump(params, f, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WFnaH40bI94X",
        "outputId": "03fb1ead-7a49-4dca-cd32-287927dc7507"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 4.16\n",
            "| 에폭 1 |  반복 3001 / 9295 | 시간 115[s] | 손실 2.65\n",
            "| 에폭 1 |  반복 6001 / 9295 | 시간 227[s] | 손실 2.40\n",
            "| 에폭 1 |  반복 9001 / 9295 | 시간 340[s] | 손실 2.29\n",
            "| 에폭 2 |  반복 1 / 9295 | 시간 351[s] | 손실 2.23\n",
            "| 에폭 2 |  반복 3001 / 9295 | 시간 460[s] | 손실 2.16\n",
            "| 에폭 2 |  반복 6001 / 9295 | 시간 570[s] | 손실 2.11\n",
            "| 에폭 2 |  반복 9001 / 9295 | 시간 679[s] | 손실 2.06\n",
            "| 에폭 3 |  반복 1 / 9295 | 시간 690[s] | 손실 2.03\n",
            "| 에폭 3 |  반복 3001 / 9295 | 시간 800[s] | 손실 1.96\n",
            "| 에폭 3 |  반복 6001 / 9295 | 시간 911[s] | 손실 1.95\n",
            "| 에폭 3 |  반복 9001 / 9295 | 시간 1022[s] | 손실 1.92\n",
            "| 에폭 4 |  반복 1 / 9295 | 시간 1033[s] | 손실 1.91\n",
            "| 에폭 4 |  반복 3001 / 9295 | 시간 1142[s] | 손실 1.84\n",
            "| 에폭 4 |  반복 6001 / 9295 | 시간 1250[s] | 손실 1.83\n",
            "| 에폭 4 |  반복 9001 / 9295 | 시간 1358[s] | 손실 1.82\n",
            "| 에폭 5 |  반복 1 / 9295 | 시간 1368[s] | 손실 1.81\n",
            "| 에폭 5 |  반복 3001 / 9295 | 시간 1477[s] | 손실 1.75\n",
            "| 에폭 5 |  반복 6001 / 9295 | 시간 1587[s] | 손실 1.75\n",
            "| 에폭 5 |  반복 9001 / 9295 | 시간 1696[s] | 손실 1.74\n",
            "| 에폭 6 |  반복 1 / 9295 | 시간 1707[s] | 손실 1.74\n",
            "| 에폭 6 |  반복 3001 / 9295 | 시간 1815[s] | 손실 1.67\n",
            "| 에폭 6 |  반복 6001 / 9295 | 시간 1922[s] | 손실 1.67\n",
            "| 에폭 6 |  반복 9001 / 9295 | 시간 2028[s] | 손실 1.67\n",
            "| 에폭 7 |  반복 1 / 9295 | 시간 2039[s] | 손실 1.66\n",
            "| 에폭 7 |  반복 3001 / 9295 | 시간 2147[s] | 손실 1.60\n",
            "| 에폭 7 |  반복 6001 / 9295 | 시간 2255[s] | 손실 1.61\n",
            "| 에폭 7 |  반복 9001 / 9295 | 시간 2363[s] | 손실 1.61\n",
            "| 에폭 8 |  반복 1 / 9295 | 시간 2374[s] | 손실 1.61\n",
            "| 에폭 8 |  반복 3001 / 9295 | 시간 2480[s] | 손실 1.54\n",
            "| 에폭 8 |  반복 6001 / 9295 | 시간 2586[s] | 손실 1.55\n",
            "| 에폭 8 |  반복 9001 / 9295 | 시간 2693[s] | 손실 1.56\n",
            "| 에폭 9 |  반복 1 / 9295 | 시간 2703[s] | 손실 1.57\n",
            "| 에폭 9 |  반복 3001 / 9295 | 시간 2811[s] | 손실 1.50\n",
            "| 에폭 9 |  반복 6001 / 9295 | 시간 2919[s] | 손실 1.51\n",
            "| 에폭 9 |  반복 9001 / 9295 | 시간 3027[s] | 손실 1.51\n",
            "| 에폭 10 |  반복 1 / 9295 | 시간 3038[s] | 손실 1.53\n",
            "| 에폭 10 |  반복 3001 / 9295 | 시간 3144[s] | 손실 1.45\n",
            "| 에폭 10 |  반복 6001 / 9295 | 시간 3251[s] | 손실 1.46\n",
            "| 에폭 10 |  반복 9001 / 9295 | 시간 3357[s] | 손실 1.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model eval\n",
        "from common.util import most_similar\n",
        "\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "  params = pickle.load(f)\n",
        "  word_vecs = params['word_vecs']\n",
        "  word_to_id = params['word_to_id']\n",
        "  id_to_word = params['id_to_word']\n",
        "\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "  most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "id": "hS8eJoYeO5gC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2480de31-490f-46f8-a738-20f49bd170bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.71630859375\n",
            " i: 0.6796875\n",
            " your: 0.5966796875\n",
            " someone: 0.57568359375\n",
            " they: 0.57373046875\n",
            "\n",
            "[query] year\n",
            " month: 0.85400390625\n",
            " week: 0.77099609375\n",
            " spring: 0.76904296875\n",
            " summer: 0.74755859375\n",
            " decade: 0.66796875\n",
            "\n",
            "[query] car\n",
            " luxury: 0.6181640625\n",
            " auto: 0.59375\n",
            " truck: 0.57958984375\n",
            " cars: 0.56982421875\n",
            " window: 0.56689453125\n",
            "\n",
            "[query] toyota\n",
            " honda: 0.6376953125\n",
            " engines: 0.62841796875\n",
            " chevrolet: 0.625\n",
            " seita: 0.60498046875\n",
            " f-14: 0.6044921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram:\n",
        "  def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "      V, H = vocab_size, hidden_size\n",
        "      rn = np.random.randn\n",
        "\n",
        "      # 가중치 초기화\n",
        "      W_in = 0.01 * rn(V, H).astype('f')\n",
        "      W_out = 0.01 * rn(V, H).astype('f')\n",
        "\n",
        "      # 계층 생성\n",
        "      self.in_layer = Embedding(W_in)\n",
        "      self.loss_layers = []\n",
        "      for i in range(2 * window_size):\n",
        "          layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "          self.loss_layers.append(layer)\n",
        "\n",
        "      # 모든 가중치와 기울기를 배열모음.\n",
        "      layers = [self.in_layer] + self.loss_layers\n",
        "      self.params, self.grads = [], []\n",
        "      for layer in layers:\n",
        "          self.params += layer.params\n",
        "          self.grads += layer.grads\n",
        "\n",
        "      # 인스턴스 변수에 단어의 분산 표현을 저장\n",
        "      self.word_vecs = W_in\n",
        "\n",
        "  def forward(self, contexts, target):\n",
        "      h = self.in_layer.forward(target)\n",
        "\n",
        "      loss = 0\n",
        "      for i, layer in enumerate(self.loss_layers):\n",
        "          loss += layer.forward(h, contexts[:, i])\n",
        "      return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "      dh = 0\n",
        "      for i, layer in enumerate(self.loss_layers):\n",
        "          dh += layer.backward(dout)\n",
        "      self.in_layer.backward(dh)\n",
        "      return None"
      ],
      "metadata": {
        "id": "XcWlCCNAR_b4"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}